<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Nicoline Nymand-Andersen</title>
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <header>
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.0/css/all.min.css">
        <img src="images/Portrait.PNG">
        <br>
        <h1>Nicoline Rosa Westrin Nymand-Andersen</h1>
        <h2 style="font-size: 19px;color:#494949;">M.Sc. Computer Science and M.Sc. Artificial Intelligence</h2>
    </header>

    <main>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async
            src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
        </script>
        <section class="about"  style="left: 4%;">
            <div class="box-container">
                <div class="info-box">
                    <h2>The K-NN Chain Algorithm: Scalable Hierarchical Agglomerative Clustering through Dynamic Nearest Neighbour Calculations</h2>
                    <br>
                    <p>
                        <b>Abstract.</b> Hierarchical clustering is a leading technique for grouping similar objects in datasets, offering the distinct advantage of detecting these patterns at multiple levels of granularity. This makes it particularly valuable in fields such as biology, finance, and social sciences, where understanding relationships across different scales is crucial. However, the high time and memory complexities of hierarchical clustering algorithms, particularly when scaling to large datasets, pose significant challenges. Optimised algorithms like NN Chain improve upon traditional methods but still face bottlenecks with quadratic time and memory complexity \(O(n^2)\) since it requires the computation and storage of a distance matrix.
                        This paper introduces a new approach, the K-NN chain, an algorithm, which reduces time and memory requirements by dynamically computing distances and storing only the \(k\)-nearest neighbours at each step in the chain. The memory complexity is \(O(n)\), making the algorithm significantly more scalable compared to alternatives. Additionally, the K-NN chain algorithm enhances flexibility by incorporating sample weights, allowing for the adjustment of the relative importance or stability of each cluster.
                    </p>
                    <br>
                    <h3>Introduction</h3>
                    <p></p>

                    <br>
                    <h3>Hierarchical agglomerative clustering methods</h3>
                    <p></p>

                    <br>
                    <h3>K-NN Chain Algorithm</h3>
                    <p></p>

                    <br>
                    <h3>Datasets</h3>
                    <p></p>

                    <br>
                    <h3>Experiments and Results</h3>
                    <p></p>

                    <br>
                    <h3>Limitations</h3>
                    <p></p>

                    <br>
                    <h3>Conclusion</h3>
                    <p></p>
                </div>
            </div>
        </section>

        

    <footer>
        <p>&copy; 2025 Nicoline R. W. Nymand-Andersen</p>
    </footer>
</body>
</html>


<!-- <!DOCTYPE html>
<html>
<head>
  <title>LaTeX in HTML</title>
  
</head>
<body>
  <h2>Inline LaTeX: \( a^2 + b^2 = c^2 \)</h2>
  
  <h2>Block LaTeX:</h2>
  <p>
    \[
    \int_0^\infty e^{-x^2} dx = \frac{\sqrt{\pi}}{2}
    \]
  </p>
</body>
</html> -->